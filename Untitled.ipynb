{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as ec\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(term=\"Laptop\"):\n",
    "    search_xpath = '//*[@id=\"twotabsearchtextbox\"]'\n",
    "    search_button = wait.until(ec.element_to_be_clickable((By.XPATH, search_xpath)))\n",
    "    search_button.send_keys(term)\n",
    "    search_button.send_keys(Keys.ENTER)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    term = input(\"Enter product : \")\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(\"https://www.amazon.in\")\n",
    "    wait = WebDriverWait(browser, 600)\n",
    "    search(term)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def search(term=\"Laptop\"):\n",
    "    search_xpath = '//*[@id=\"twotabsearchtextbox\"]'\n",
    "    search_button = wait.until(ec.element_to_be_clickable((By.XPATH, search_xpath)))\n",
    "    search_button.send_keys(term)\n",
    "    search_button.send_keys(Keys.ENTER)\n",
    "\n",
    "def getProducts():\n",
    "    products = soup.find_all(\"a\", {\"class\":\"a-link-normal s-no-outline\"})\n",
    "    if len(products)>0:\n",
    "        return products\n",
    "    return []\n",
    "\n",
    "def clickNext(browser):\n",
    "    xpath = '//li[@class=\"a-last\"]'\n",
    "    btn = wait.until(ec.element_to_be_clickable((By.XPATH, xpath)))\n",
    "    btn.click()\n",
    "\n",
    "def brandName(soup):\n",
    "    try:\n",
    "        return soup.find(text=\"Brand\").findNext(\"td\").get_text().strip()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def prodName(soup):\n",
    "    try:\n",
    "        data = soup.find(\"span\",{\"id\":\"productTitle\"}).get_text()\n",
    "        data = re.sub(\"\\s+\",\" \",data)\n",
    "        return data.strip()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def getRating(soup):\n",
    "    try:\n",
    "        data = soup.find(\"span\",{\"class\":\"a-icon-alt\"}).get_text().split(\" \")[0]\n",
    "        if \"Previous\" in data:\n",
    "            return \"-\"\n",
    "        else:\n",
    "            return data\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def numRatings(soup):\n",
    "    try:\n",
    "        return soup.find(\"span\",{\"id\":\"acrCustomerReviewText\"}).get_text().split(\" \")[0]\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def getPrice(soup):\n",
    "    try:\n",
    "        return soup.find(\"span\",{\"id\":\"priceblock_ourprice\"}).get_text()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def returnExchange(soup):\n",
    "    return bool(soup.find(\"a\",{\"class\":\"a-size-small a-link-normal a-text-normal\"}).get_text())\n",
    "\n",
    "def expectedDelivery(soup):\n",
    "    try:\n",
    "        div = soup.find(\"div\",{\"id\":\"ddmDeliveryMessage\"})\n",
    "        return div.find(\"b\").get_text().strip()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def availability(soup):\n",
    "    text = soup.get_text().lower()\n",
    "    return bool(re.search(\"in stock\", text))\n",
    "\n",
    "def otherDetails(soup):\n",
    "    try:\n",
    "        data = soup.find(\"div\",{\"id\":\"productDescription\"}).get_text()\n",
    "        data = re.sub(\"\\s+\",\" \",data)\n",
    "        return data.strip()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def report(brand_name, prod_name, rating, num_of_ratings, price, return_exchange, expected_delivery, avail, other_details,prod_url):\n",
    "    df = pd.read_excel(\"DataQ2.xlsx\")\n",
    "\n",
    "    #Saving data into excel sheet\n",
    "    data = {\"Brand Name\":[brand_name], \"Product Name\":[prod_name],\"Rating\":[rating],\n",
    "            \"No. of Ratings\":[num_of_ratings],\"Price\":[price],\"Return/Exchange\":[return_exchange],\n",
    "            \"Expected Delivery\":[expected_delivery],\"Availability\":[avail],\n",
    "            \"Other Details\":[other_details],\"Product URL\":[prod_url]\n",
    "            }\n",
    "    df = df.append(pd.DataFrame(data))\n",
    "    writer = pd.ExcelWriter(\"DataQ2.xlsx\")\n",
    "    df.to_excel(writer,index=False)\n",
    "    writer.save()\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    term = input(\"Enter product : \")\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(\"https://www.amazon.in\")\n",
    "    wait = WebDriverWait(browser, 600)\n",
    "    search(term)\n",
    "\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    products = []\n",
    "    for i in range(3):\n",
    "        products=products+getProducts()\n",
    "        try:\n",
    "            clickNext(browser)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    for product in products:\n",
    "        prod_url = \"https://www.amazon.in\"+product[\"href\"]\n",
    "        browser.get(prod_url)\n",
    "        html = browser.page_source\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "        brand_name = brandName(soup)\n",
    "        prod_name = prodName(soup)\n",
    "        rating = getRating(soup)\n",
    "        num_of_ratings = numRatings(soup)\n",
    "        price = getPrice(soup)\n",
    "        return_exchange = returnExchange(soup)\n",
    "        expected_delivery = expectedDelivery(soup)\n",
    "        avail = availability(soup)\n",
    "        other_details = otherDetails(soup)\n",
    "        \"\"\"\n",
    "        print(brand_name)\n",
    "        print(prod_name)\n",
    "        print(rating)\n",
    "        print(num_of_ratings)\n",
    "        print(price)\n",
    "        print(return_exchange)\n",
    "        print(expected_delivery)\n",
    "        print(avail)\n",
    "        print(other_details)\n",
    "        \"\"\"\n",
    "        report(brand_name, prod_name, rating, num_of_ratings, price, return_exchange, expected_delivery, avail, other_details, prod_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    search_xpath = '//input[@class=\"gLFyf gsfi\"]'\n",
    "    search_button = wait.until(ec.element_to_be_clickable((By.XPATH, search_xpath)))\n",
    "    search_button.send_keys(query)\n",
    "    search_button.send_keys(Keys.ENTER)\n",
    "\n",
    "def getURLs(soup):\n",
    "    div = soup.find(\"div\",{\"class\":\"islrc\"})\n",
    "    imgs = div.find_all(\"img\",limit=100)\n",
    "    urls = []\n",
    "    for img in imgs:\n",
    "        urls.append(img[\"src\"])\n",
    "    return urls\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    browser = webdriver.Chrome()\n",
    "    wait = WebDriverWait(browser, 600)\n",
    "    queries = [\"fruits\", \"cars\", \"Machine Learning\"]\n",
    "\n",
    "    for query in queries:\n",
    "        print(\"Now searching for {}\".format(query))\n",
    "        browser.get(\"https://images.google.com/\")\n",
    "        search(query)\n",
    "        html = browser.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        urls = getURLs(soup)\n",
    "        for i,url in enumerate(urls):\n",
    "            file_name = \"./Q3-images/\"+query+\"/\"+str(i)+\".jpg\"\n",
    "            urllib.request.urlretrieve(url, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brandName(soup):\n",
    "    try:\n",
    "        return soup.find(\"span\",{\"class\":\"B_NuCI\"}).get_text().strip().split()[0]\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def phoneName(soup):\n",
    "    try:\n",
    "        return soup.find(\"span\",{\"class\":\"B_NuCI\"}).get_text().strip()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def color(soup):\n",
    "    try:\n",
    "        return soup.find(text=\"Color\").findNext(\"td\").get_text().strip()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def ram(soup):\n",
    "    try:\n",
    "        return soup.find(text=\"Ram Capacity\").findNext(\"td\").get_text().strip()\n",
    "    except:\n",
    "        try:\n",
    "            return soup.find(text=\"RAM\").findNext(\"td\").get_text().strip()\n",
    "        except:\n",
    "            return \"-\"\n",
    "\n",
    "def storage(soup):\n",
    "    try:\n",
    "        return soup.find(text=\"Storage Memory\").findNext(\"td\").get_text().strip()\n",
    "    except:\n",
    "        try:\n",
    "            return soup.find(text=\"Internal Storage\").findNext(\"td\").get_text().strip()\n",
    "        except:\n",
    "            return \"-\"\n",
    "\n",
    "def primaryCamera(soup):\n",
    "    try:\n",
    "        return soup.find(text=\"Primary Camera\").findNext(\"td\").get_text().strip()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def secondCamera(soup):\n",
    "    try:\n",
    "        return soup.find(text=\"Secondary Camera\").findNext(\"td\").get_text().strip()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def displaySize(soup):\n",
    "    try:\n",
    "        return soup.find(text=\"Display Size\").findNext(\"td\").get_text().strip()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def displayResol(soup):\n",
    "    try:\n",
    "        return soup.find(text=\"Resolution\").findNext(\"td\").get_text().strip()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def processor(soup):\n",
    "    try:\n",
    "        return soup.find(text=\"Processor Type\").findNext(\"td\").get_text().strip()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def cores(soup):\n",
    "    try:\n",
    "        return soup.find(text=\"Processor Core\").findNext(\"td\").get_text().strip()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def battery(soup):\n",
    "    try:\n",
    "        return soup.find(text=\"Battery Capacity\").findNext(\"td\").get_text().strip()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def price(soup):\n",
    "    try:\n",
    "        return soup.find(\"div\",{\"class\":\"_30jeq3 _16Jk6d\"}).get_text().strip()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def report(brand, name, col, rm, rom, pri_cam, sec_cam, dis_size,\n",
    "                des_res, proc, core, bat, ruppee, prod_url):\n",
    "    df = pd.read_excel(\"Q4.xlsx\")\n",
    "    #Saving data into excel sheet\n",
    "    data = {\"Brand Name\":[brand], \"Product Name\":[name],\"Color\":[col],\n",
    "            \"RAM\":[rm],\"ROM\":[rom],\"Primary Camera\":[pri_cam],\n",
    "            \"Secondary Camera\":[sec_cam],\"Display Size\":[dis_size],\n",
    "            \"Display Resolution\":[des_res],\"Processor\":[proc],\"Cores\":[core],\n",
    "            \"Battery\":[bat],\"Price\":[ruppee],\"Product URL\":[prod_url]\n",
    "            }\n",
    "    df = df.append(pd.DataFrame(data))\n",
    "    writer = pd.ExcelWriter(\"Q4.xlsx\")\n",
    "    df.to_excel(writer,index=False)\n",
    "    writer.save()\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(\"https://www.flipkart.com\")\n",
    "    wait = WebDriverWait(browser, 600)\n",
    "\n",
    "    #close button\n",
    "    xpath = '//button[@class=\"_2KpZ6l _2doB4z\"]'\n",
    "    btn = browser.find_element_by_xpath(xpath)\n",
    "    try:\n",
    "        btn.click()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    #search\n",
    "    xpath = '//input[@title=\"Search for products, brands and more\"]'\n",
    "    btn = browser.find_element_by_xpath(xpath)\n",
    "    btn.send_keys(\"Oneplus Nord\")\n",
    "    btn.send_keys(Keys.ENTER)\n",
    "\n",
    "    #get all a's\n",
    "    time.sleep(5)\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    a_tags = soup.find_all(\"a\",{\"class\":\"_1fQZEK\"})\n",
    "    urls = []\n",
    "    for a in a_tags:\n",
    "        prod_url = \"https://www.flipkart.com\"+a[\"href\"]\n",
    "        urls.append(prod_url)\n",
    "\n",
    "    #Iterate through each url\n",
    "    for url in urls:\n",
    "        browser.get(url)\n",
    "        html = browser.page_source\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        brand = brandName(soup)\n",
    "        name = phoneName(soup)\n",
    "        col = color(soup)\n",
    "        rm = ram(soup)\n",
    "        rom = storage(soup)\n",
    "        pri_cam = primaryCamera(soup)\n",
    "        sec_cam = secondCamera(soup)\n",
    "        dis_size = displaySize(soup)\n",
    "        des_res = displayResol(soup)\n",
    "        proc= processor(soup)\n",
    "        core = cores(soup)\n",
    "        bat = battery(soup)\n",
    "        ruppee = price(soup)\n",
    "\n",
    "        report(brand, name, col, rm, rom, pri_cam, sec_cam, dis_size,\n",
    "                des_res, proc, core, bat, ruppee, prod_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(\"https://trak.in/\")\n",
    "    wait = WebDriverWait(browser, 600)\n",
    "    \n",
    "    #Goto Funding Deals\n",
    "    xpath = '//li[@id=\"menu-item-51510\"]'\n",
    "    btn = wait.until(ec.element_to_be_clickable((By.XPATH,xpath)))\n",
    "    btn.click()\n",
    "\n",
    "    time.sleep(5)\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "    #Get data for July\n",
    "    table_july = soup.find(\"table\",{\"id\":\"tablepress-48\"})\n",
    "    df = pd.read_html(str(table_july))\n",
    "    print(\"July\")\n",
    "    print(df)\n",
    "    print()\n",
    "\n",
    "    #Get data for August\n",
    "    table_aug = soup.find(\"table\",{\"id\":\"tablepress-49\"})\n",
    "    df = pd.read_html(str(table_aug))\n",
    "    print(\"August\")\n",
    "    print(df)\n",
    "    print()\n",
    "\n",
    "    #Get data for September\n",
    "    table_sep = soup.find(\"table\",{\"id\":\"tablepress-50\"})\n",
    "    df = pd.read_html(str(table_sep))\n",
    "    print(\"September\")\n",
    "    print(df)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    browser = webdriver.Chrome()\n",
    "    wait = WebDriverWait(browser, 600)\n",
    "\n",
    "    #Goto Homepage\n",
    "    browser.get(\"https://www.digit.in/\")\n",
    "\n",
    "    #Goto Gaming Tab\n",
    "    browser.get(\"https://www.digit.in/gaming/\")\n",
    "\n",
    "    #Goto Gaming mart tab\n",
    "    browser.get(\"https://www.digit.in/gaming-mart/\")\n",
    "\n",
    "    #Goto Gaming Laptops\n",
    "    browser.get(\"https://www.digit.in/gaming-mart/topten/gaming-laptops/\")\n",
    "\n",
    "    #Click on Read more button\n",
    "    xpath = '//a[@class=\"review\"]'\n",
    "    btn = wait.until(ec.element_to_be_clickable((By.XPATH, xpath)))\n",
    "    btn.click()\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "    #Get all product links\n",
    "    urls = soup.find_all(\"a\",{\"class\":\"review spec\"})\n",
    "\n",
    "    for i,url in enumerate(urls,start=1):\n",
    "        print(\"Getting data for product {}\".format(i))\n",
    "        print()\n",
    "        browser.get(url[\"href\"])\n",
    "        time.sleep(3)\n",
    "        html = browser.page_source\n",
    "        soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "        details = soup.find(\"div\",{\"class\":\"tabcontent\"}).get_text().strip()\n",
    "        details = re.sub(\"\\s\\s+\",\"\\n\",details)\n",
    "        details = re.sub(\"\\s:\\s\",\":\",details)\n",
    "\n",
    "        print(details)\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(rank, name, net_worth, age, citizen, source, industry):\n",
    "    df = pd.read_excel(\"Q8.xlsx\")\n",
    "    #Saving data into excel sheet\n",
    "    data = {\"Rank\":[rank],\"Name\":[name],\"Net Worth\":[net_worth],\n",
    "            \"Age\":[age],\"Citizenship\":[citizen],\"Source\":[source],\n",
    "            \"Industry\":[industry]\n",
    "            }\n",
    "    df = df.append(pd.DataFrame(data))\n",
    "    writer = pd.ExcelWriter(\"Q8.xlsx\")\n",
    "    df.to_excel(writer,index=False)\n",
    "    writer.save()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    browser = webdriver.Chrome()\n",
    "    wait = WebDriverWait(browser, 600)\n",
    "\n",
    "    browser.get(\"https://www.forbes.com\")\n",
    "    browser.execute_script(\"window.close();\")\n",
    "\n",
    "    #Goto Billionaires list\n",
    "    browser.get(\"https://www.forbes.com/billionaires/\")\n",
    "    time.sleep(3)\n",
    "\n",
    "    ranks = browser.find_elements_by_class_name(\"rank\")\n",
    "    names = browser.find_elements_by_class_name(\"personName\")\n",
    "    nets = browser.find_elements_by_class_name(\"netWorth\")\n",
    "    ages = browser.find_elements_by_class_name(\"age\")\n",
    "    citizens = browser.find_elements_by_class_name(\"countryOfCitizenship\")\n",
    "    sources = browser.find_elements_by_class_name(\"source\")\n",
    "    categories = browser.find_elements_by_class_name(\"category\")\n",
    "\n",
    "    for rank,name,net,age,citizen,source,category in zip(ranks,names,nets,ages,citizens,sources,categories):\n",
    "        print(rank.text, name.text, net.text, age.text, citizen.text, source.text, category.text)\n",
    "        print()\n",
    "        print()\n",
    "        report(rank.text, name.text, net.text, age.text, citizen.text, source.text, category.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(comment,upvote,time):\n",
    "    df = pd.read_excel(\"Q9.xlsx\")\n",
    "\n",
    "    #Saving data into excel sheet\n",
    "    data = {\"Comment\":[comment],\"Upvotes\":[upvote],\"Time\":[time]}\n",
    "    df = df.append(pd.DataFrame(data))\n",
    "    writer = pd.ExcelWriter(\"Q9.xlsx\")\n",
    "    df.to_excel(writer,index=False)\n",
    "    writer.save()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(\"https://www.youtube.com/watch?v=Pkh8UtuejGw\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    count=1\n",
    "    while(count<=50):\n",
    "        if count==1:\n",
    "            browser.find_element_by_tag_name(\"body\").send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(3)\n",
    "        time.sleep(2)\n",
    "        browser.find_element_by_tag_name(\"body\").send_keys(Keys.END)\n",
    "        count+=1\n",
    "\n",
    "    browser.find_element_by_tag_name(\"body\").send_keys(Keys.PAGE_UP)\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    tags = soup.find_all(\"ytd-comment-renderer\",{\"id\":\"comment\"},limit=500)\n",
    "    upvotes = soup.find_all(\"span\",{\"id\":\"vote-count-middle\"},limit=500)\n",
    "    times = soup.find_all(\"yt-formatted-string\",{\"class\":\"published-time-text above-comment style-scope ytd-comment-renderer\"},limit=500)\n",
    "    print(len(tags), len(upvotes), len(times))\n",
    "    for tag, upvote, t in zip(tags, upvotes, times):\n",
    "        report(re.sub(\"\\s+\",\" \",tag.text),\n",
    "                re.sub(\"\\s+\",\" \",upvote.text), t.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getName(div):\n",
    "    try:\n",
    "        return div.find(\"a\").get_text()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def getDistance(div):\n",
    "    try:\n",
    "        return div.find(\"span\",{\"class\":\"description\"}).get_text()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def getRatings(div):\n",
    "    try:\n",
    "        return div.find(\"div\",{\"class\":\"score orange big\"}).get_text()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def getTotalReviews(div):\n",
    "    try:\n",
    "        return div.find(\"div\",{\"class\":\"reviews\"}).get_text()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def getOverallReviews(div):\n",
    "    try:\n",
    "        return div.find(\"div\",{\"class\":\"keyword\"}).get_text()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def privatePrice(div):\n",
    "    try:\n",
    "        return div.find(\"div\",{\"class\":\"price title-5\"}).get_text()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def dormsPrice(div):\n",
    "    try:\n",
    "        return div.find(\"div\",{\"class\":\"price title-5\"}).findNext(\"div\").get_text()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def getFacilities(div):\n",
    "    try:\n",
    "        return div.find(\"div\",{\"class\":\"facilities-label facilities\"}).get_text()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "def getDescription(div):\n",
    "    try:\n",
    "        return div.find(\"div\",{\"class\":\"rating-factors prop-card-tablet rating-factors small\"}).get_text()\n",
    "    except:\n",
    "        return \"-\"\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    browser = webdriver.Chrome()\n",
    "    browser.get(\"https://www.hostelworld.com/s?q=London,%20England&country=England&city=London&type=city&id=3&from=2021-06-27&to=2021-06-30&guests=2&page=1\")\n",
    "    wait = WebDriverWait(browser, 600)  \n",
    "\n",
    "    time.sleep(10)\n",
    "\n",
    "    html = browser.page_source\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "\n",
    "    divs = soup.find_all(\"div\",{\"class\":\"property-card\"})\n",
    "    divs.pop(0)\n",
    "    divs.pop(0)\n",
    "\n",
    "    for div in divs:\n",
    "        name = getName(div)\n",
    "        distance = getDistance(div)\n",
    "        ratings = getRatings(div)\n",
    "        total_reviews = getTotalReviews(div)\n",
    "        overall_reviews = getOverallReviews(div)\n",
    "        privates_price = privatePrice(div)\n",
    "        dorms_price = dormsPrice(div)\n",
    "        facilities = getFacilities(div)\n",
    "        description = getDescription(div)\n",
    "\n",
    "        name = re.sub(\"\\s+\",\" \",name)\n",
    "        distance = re.sub(\"\\s+\",\" \",distance)\n",
    "        ratings = re.sub(\"\\s+\",\" \",ratings)\n",
    "        total_reviews = re.sub(\"\\s+\",\" \",total_reviews)\n",
    "        overall_reviews = re.sub(\"\\s+\",\" \",overall_reviews)\n",
    "        privates_price = re.sub(\"\\s+\",\" \",privates_price)\n",
    "        dorms_price = re.sub(\"\\s+\",\" \",dorms_price)\n",
    "        facilities = re.sub(\"\\s+\",\" \",facilities)\n",
    "        description = re.sub(\"\\s+\",\" \",description)\n",
    "\n",
    "\n",
    "        print(name)\n",
    "        print(distance)\n",
    "        print(ratings)\n",
    "        print(total_reviews)\n",
    "        print(overall_reviews)\n",
    "        print(privates_price)\n",
    "        print(dorms_price)\n",
    "        print(facilities)\n",
    "        print(description)\n",
    "        print()\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
